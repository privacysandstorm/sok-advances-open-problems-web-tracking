%%
\section{Measurement Methodologies}
\label{sec:measurement-methodologies}

%%
To measure web tracking, researchers have developed a multitude of crawling tools and carried out several user studies over the years. Next, we discuss a few examples to illustrate the diversity and associated limitations of these approaches.


%%
\subsection{Crawling Measurements}
\label{sec:crawling}

Web crawling is by far the most common approach to measuring online tracking. It allows researchers to write custom instrumentation to collect the most recent behavior of trackers. There are several dimensions across which tooling decisions can be made, depending on the measurement needs. Browser instrumentation can take two forms: out-of-band or in-band. Out-of-band instrumentation, or deep instrumentation, modifies directly the browser or JavaScript engine to monitor for the required information of the web measurement. In contrast, in-band instrumentation leverages instrumentation hooks, like prototype patching, at the JavaScript level to overwrite functionality of interest. 

\paragraph{User Agent.} Most web tracking measurements require a full consumer browser, as trackers are frequently loaded into pages dynamically using JavaScript and can make use of the many storage mechanisms and DOM interfaces exposed by browsers. Simplified user agents which don't execute JavaScript (\eg{}, Apache Nutch~\cite{apache-nutch}) or which have incomplete support for Web APIs can be appropriate for targeted measurements, such as the use of HTTP headers on the index page of sites. 

\paragraph{Automation Frameworks with Instrumentation Hooks.} To drive full consumer browsers, researchers rely on browser automation tooling built for website and browser testing. At the engine level, each browser vendor implements custom automation protocols: \eg{}, Chrome DevTools Protocol (CDP) for Blink-based browsers and Marionette for Gecko-based browsers. These internal interfaces are used by cross-browser automation libraries like Selenium or Puppeteer which implement the WebDriver or more recent WebDriver BiDi protocols~\cite{cross-browser-testing-1-2020,cross-browser-testing-2-2021,puppeteer-support-firefox-2024}. While many researchers make direct use of these APIs and libraries, there have also been several projects which bundle full browser automation with additional instrumentation and tooling built for web tracking measurement. Popular crawlers include  OpenWPM~\cite{englehardtOnlineTracking1millionsite2016}, used by hundreds of studies~\cite{openwpm4StudiesUsing}, FourthParty~\cite{mayerFourthpartyFourthpartyFourthParty2011}, DuckDuckGo's Tracker Radar Collector~\cite{duckduckgoDuckduckgoTrackerradarcollectorModular2020}, or WebXRay~\cite{libertWebXrayPrivacySearch2024}.

\paragraph{Deep Instrumentation.} 
Many attempts have been made in leveraging deep instrumentation for security-related web measurements~\cite{neasbittWebCapsuleLightweightForensic2015,liJSgraphEnablingReconstruction2018, acarFPDetectiveDustingWeb2013, IncontextStoreIncontext1998, chenMystiqueUncoveringInformation2018}. The fundamental problem is that the browser evolves rapidly, rendering the research prototypes obsolete quickly, as maintaining the patches is difficult (or impossible, Chrome 58 shipped a brand-new compiler pipeline with Ignition and Turbofan, rendering all previous taint-analysis engines, like Mystique~\cite{chenMystiqueUncoveringInformation2018} obsolete and due for a complete re-implementation). There are two major efforts that try to overcome this limitation: VisibleV8~\cite{jueckstockVisibleV8InbrowserMonitoring2019} and PageGraph~\cite{bravePageGraph,braveBravePagegraphcrawl2025}. PageGraph is maintained directly by the Brave Browser team, making it the only deep instrumentation framework that has browser support. WebREC~\cite{hantkeWebExecutionBundles2025} is a recently developed web measurement tool  built on top of PageGraph. VisibleV8 is designed in such a way so that its patches are minimal (67 lines of code for the actual JavaScript monitoring) and has been successful in providing builds from Chromium 63 to 135 (current version at submission time) with minimal effort~\cite{wspratncsuWsprncsuVisiblev82025, jueckstockVisibleV8InbrowserMonitoring2019}. One of the major benefits of deep instrumentation is that it can be agnostic to what needs to be monitored, since at the browser's source code level you can hook \textit{all} web APIs, which can enable studies that do not know the responsible APIs beforehand~\cite{suAutomaticDiscoveryEmerging2023}.

\paragraph{Stealthiness.}
One of the most significant threats to the validity of active web measurements is the ability of websites to detect automated crawlers and instrumented browsers, and subsequently alter their behavior (a practice known as \textit{cloaking}) or block access entirely~\cite{invernizzi2016cloak}. Automation frameworks, like Selenium, often inject in the JavaScript context artifacts like \texttt{navigator.webdriver} or alter the user-agent (Puppeteer changes \texttt{Chrome} to \texttt{HeadlessChrome}) which can be used by the visiting website to identify the presence of a measurement, and require researchers to deploy further evasion techniques~\cite{berstendPuppeteerextrapluginstealth2023} to avoid differential treatment.

\paragraph{Website Interactions.}
To evaluate the behavior of different web actors, measurements studies not only visit websites, but also perform additional actions, such as scrolling webpages, clicking links, completing forms, etc. These serve different and often complimentary purposes; mimicking real users' behaviors, ensuring that all resources on webpages are loaded, or performing interactions specific to the study like when evaluating cookies consent banners.

\paragraph{Site lists.}
Researchers often crawl websites sampled from top lists of popular websites published by several sources based on different measurement methodologies and viewpoints; common lists include Alexa Top Million~\cite{amazonAlexa} (now deprecated), Cisco Umbrella Popularity List~\cite{ciscoCiscoUmbrellaPopularity}, Majestic Million~\cite{majesticMajesticMillion}, Tranco~\cite{lepochatTrancoResearchOrientedTop2019}, Google CrUX~\cite{googleChromeUXReport2017}, or Cloudflare Radar~\cite{cloudflareCloudflareRadar2025} datasets. If some lists provide a total ordering of the top 1 million websites, others bin websites per popularity rank. Prior research raised some skepticism about the use of top lists as a proxy to study websites popularity and real users' browsing behaviors, as these lists are often found to be unstable over time, inconsistent among each other, and prone to manipulation. Moreover, the choice of top list can sometimes impact research findings~\cite{lepochatTrancoResearchOrientedTop2019,ruthTopplingTopLists2022}.

\paragraph{Existing Crawl Datasets.}
Another strategy to perform web measurements is to leverage already existing datasets or artifacts about web crawls and query these for the relevant information. Nonprofit organizations and community-driven projects such as the Internet Archive~\cite{InternetArchiveDigital}, Common Crawl~\cite{commoncrawlCommonCrawlOpen}, and the HTTP Archive~\cite{httparchiveHTTPArchive} routinely crawl, capture, and publish their data openly.

\paragraph{Limitations.} Web measurements have representativeness and generalizability issues due to the potential differences in results obtained from crawls versus real browsing by humans~\cite{zeberRepresentativenessAutomatedWeb2020}, the possibility that automated browser get detected by websites~\cite{krumnowHowGullibleAre2022}, as well as the impact of---among other factors---the device type (mobile vs desktop)~\cite{yangComparativeMeasurementStudy2020,casselOmniCrawlComprehensiveMeasurement2022} or the choice of country~\cite{samarasingheGlobalPerspectiveWeb2019} from which the measurements are performed. Directly related, web measurement studies are often very difficult to reproduce and replicate as differences in methodologies and experimental setups are not always fully documented by researchers~\cite{demirReproducibilityReplicabilityWeb2022,hantkeWebExecutionBundles2025}


%%
\subsection{User Studies}
\label{sec:user-studies}
%%

In practice, user studies can take multiple forms; they can be conducted through \textit{usability surveys or interviews}, or be based on data collected from real users through \textit{field measurements, crowdsourcing, or direct collection} through a browser extension or application. As an example, the National Internet Observatory~\cite{steningUnprecedentedDataCollection2022,nioNationalInternetObservatory,callahanCanWeBetter2021,fealIntroductionNationalInternet2024}, a nascent effort, invites US residents to volunteer data about their online behaviors and allows privacy-preserving access to researchers for scientific studies. 


With these techniques, researchers have mostly investigated participantsâ€™ comprehension, perception, and interaction with respect to cookie  dialogs~\cite{birrellSoKTechnicalImplementation2024,machuletzMultiplePurposesMultiple2020,bermejofernandezThisWebsiteUses2021,habibOkayWhateverEvaluation2022,singhWhatCookieConsent2022,bielovaSurveyAcademicStudies2022,Bielova2024-zr}. They typically study and compare different consent dialog designs, finding that many current designs effectively nudge participants towards more privacy-preserving options~\cite{machuletzMultiplePurposesMultiple2020,bermejofernandezThisWebsiteUses2021}. These studies also recommend that consent choices be \textit{reject by default} and that users should be able to easily revisit choices they have made~\cite{habibOkayWhateverEvaluation2022,Kanc-etal-25-PETs}.
